{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Encode a Sentence for StableDiffusion with BERT\n",
    "\n",
    "This notebook is meant to teach you about how transformers are used to encode a sentence for the LatentDiffusion model, the predecessor to StableDiffusion. The most important aspect of transformers, and all machine learning, is how we encode and decode information. We need to encode the caption in a way that can be provided to the image generation step.   \n",
    "\n",
    "This notebook focuses on how that works. We will walk through the inference pipeline of Huggingface's BERT explaining each step along the way. BERT was the start of the transformer revolution with [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf). It is the great-grandfather of ChatGPT. We're specifically using the implementation by Huggingface. The code is [here](https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py) and the documentation is [here](https://huggingface.co/docs/transformers/v4.25.1/en/model_doc/bert). StableDiffusion uses CLIP's text embedding system to do this, but the concept is the same and BERT is widely used.\n",
    "\n",
    "## Primary Components\n",
    "\n",
    "There are 4 components we need to pay attention to:\n",
    "- Tokenizer\n",
    "- Embedder\n",
    "- Transformer\n",
    "- Pooler\n",
    "\n",
    "The tokenizer and embedder work together to make the input to the transformer and transformer and pooler work together to create the useful output we need in the next step. \n",
    "\n",
    "First we need to install and import the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the huggingface transformers library, and pytorch\n",
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sven/.pyenv/versions/3.9.6/envs/bert_dissection/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries, and creating the tokenizer and model.\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "The first step is to take apart an English sentence encoded as a string. This is the job of the tokenizer. This gives us a list of 'tokens' from our sentence, represented by integers. It first splits up sentences into words and punctuation, then it splits the words into sub-words. The exact tokenizer and how it works doesn't matter that much, so don't worry about the details, but BERT uses [wordpiece](https://huggingface.co/course/chapter6/6?fw=pt). This is an attempt to learn the components of words so that our dictionary can stay small and allow words like \"snowboard\" and \"surfboard\" to be inherently related. \n",
    "\n",
    "Here's the BERT tokenizer in action, you can play with the sentence if you like to see how it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs: [101, 7592, 1010, 2026, 3899, 2003, 10140, 1012, 2002, 7777, 4586, 21172, 999, 102]\n",
      "Token Strings: ['[ C L S ]', 'h e l l o', ',', 'm y', 'd o g', 'i s', 'c u t e', '.', 'h e', 'l i k e s', 's n o w', '# # b o a r d i n g', '!', '[ S E P ]']\n",
      "Complete Decode: [CLS] hello, my dog is cute. he likes snowboarding! [SEP]\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer(\"Hello, my dog is cute. He likes snowboarding!\")\n",
    "print(\"Token IDs:\", encoding[\"input_ids\"])\n",
    "print(\"Token Strings:\", [str(tokenizer.decode(token)) for token in encoding[\"input_ids\"]])\n",
    "print(\"Complete Decode:\", tokenizer.decode(encoding[\"input_ids\"]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic and Control Tokens\n",
    "\n",
    "Note that there are 2 extra tokens that do no correspond to words in our original sentence! These are the control tokens, which are the secret sauce that makes this all work. `[CLS]` means we're doing a \"classification\" task, and `[SEP]` which means end in this case. There are a few more tokens, `[UNK]` means unknown, `[MASK]` means that this token is masked off. \n",
    "\n",
    "The `[CLS]` token is the most important for us. The BERT model was trained to do several tasks. One task was to relate two sentences based on the final state of this token, after the embedding, transformer, and pooling steps (which we go into next). BERT is trained to say that \"I have never seen a hummingbird\" contradicts the premise \"I have never seen a hummingbird not flying\" based off of what the transformer does to this token. So, it in a sense contains the \"meaning\" of the sentence. Another task it was trained to complete is to predict words/tokens that were removed which uses the `[MASK]` token. \n",
    "\n",
    "### Other Tokenizers\n",
    "\n",
    "OpenAI has a really good tool explaining their favorite tokenizer [here](https://beta.openai.com/tokenizer) that uses [byte-pair-encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding). This is more flexible than the wordpiece, but uses more tokens. If you look at the OpenAI tokenizer it splits `snowboarding` into 3 tokens, `_s`, `now`, and `boarding`, but BERT's splits it up into only 2 tokens `snow`, `##boarding` (the `##` means it's a continuation of a word). It took researchers a while to figure out that transformers could work well with a less linguistically informed tokenizer. For natural language as long as you do it sensibly, the exact tokenization doesn't matter that much for reasons that will become clear later. \n",
    "\n",
    "However, for security research, this could matter a lot. Network packets are more structured than language and a good tokenization will give the transformer a massive leg up when it's trying to learn. You could encode a lot of information about an environment in a token sequence, if you do it right. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedder\n",
    "\n",
    "The next step is to take these tokens, which are a just list of integers, and give them room to breathe. The information an integer can encode is small. There's only ~30,000 of them and they're just numbers. We need something that can contain all the information the word \"dog\" has. You as a human have a picture in your mind of a mammal that has 4 legs and a head. You may remember pets you grew up with, but a ML system doesn't have any of that. A number cannot contain that much information, so we give it 768 floating point numbers instead with an embedding layer. \n",
    "\n",
    "#### A Warning!!!\n",
    "This is all the system has, it is a single point in space that is nearly meaningless. There's a strong desire to anthropomorphize this, which I've done a bit in this writing, but this information encoding is fundamentally different from how humans process information. This embedding has more to do with the number of bits of information we need this token and it's role in the transformer to contain than giving it a place to store semantic meaning. Think of it more like a location in a space, like a point on a map, than anything else.\n",
    "\n",
    "The model contains this embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEmbeddings(\n",
       "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (token_type_embeddings): Embedding(2, 768)\n",
       "  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 30522 elements in the `word_embeddings` component of the embedder. The embedder just takes token `[CLS]` token, which is `101`, to the 101st element of this list. \n",
    "\n",
    "However, there are some other embeddings. The `position_embeddings` encode the position of the token. The `[CLS]` token is in position 0, and so we add the position embedding for 0 to that token's embedding. The transformer does not care what order it receives it's input sequence. Adding this position information to each token is how we fix that. The token type embedding isn't used in LatentDiffusion, so we can ignore that. Note that this was only trained on sequences that have at most 512 tokens! \n",
    "\n",
    "Here's the embedding layer applied to our sentence. It's a giant tensor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.6855e-01, -2.8577e-01, -3.2613e-01,  ..., -2.7571e-02,\n",
       "           3.8253e-02,  1.6400e-01],\n",
       "         [ 3.7386e-01, -1.5575e-02, -2.4561e-01,  ..., -3.1657e-02,\n",
       "           5.5144e-01, -5.2406e-01],\n",
       "         [ 4.6706e-04,  1.6225e-01, -6.4443e-02,  ...,  4.9443e-01,\n",
       "           6.9413e-01,  3.6286e-01],\n",
       "         ...,\n",
       "         [-1.1198e-01, -1.0581e+00, -2.0782e-01,  ..., -6.7460e-01,\n",
       "          -9.2665e-02,  4.3315e-02],\n",
       "         [ 4.7513e-01, -1.6158e-01, -3.3946e-01,  ...,  5.6113e-01,\n",
       "           5.0795e-01,  5.9823e-01],\n",
       "         [-6.0564e-01,  9.6814e-02,  1.8802e-01,  ..., -2.7735e-01,\n",
       "           1.8495e-01,  5.7977e-02]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer(\"Hello, my dog is cute. He likes snowboarding!\", return_tensors=\"pt\")\n",
    "model.embeddings(encoding[\"input_ids\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important part of this is the shape of the tensor. We can print it out and inspect it. We have 1 sentence, there are 14 tokens for our sentence, and each token is embedded in 768 dimensions. So we get the shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embeddings(encoding[\"input_ids\"]).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor is just a fancy way of organizing the floating point numbers. This is a rank 3 1x14x768 tensor, which is represented in the computer as an array of 10752 floating point numbers with another 3 integers 1, 14, and 768 to determine the \"shape\". You can think of this as a list with 1 element which is a list with 14 vectors where each vector has 768 elements.  \n",
    "\n",
    "## Transformer\n",
    "\n",
    "The encoder is the transformer. It's the next step in the process. We're focused on the embeddings here, not the transformer so we're not going to focus on how this component works, but we will talk about what it does. First let's look at the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[[ 0.0233,  0.1726, -0.0896,  ..., -0.2837,  0.3139,  0.6161],\n",
      "         [ 0.5281,  0.1442,  0.0739,  ...,  0.0331,  1.0601, -0.1347],\n",
      "         [-0.4106,  0.3789,  0.5610,  ..., -0.4233,  0.7288, -0.0884],\n",
      "         ...,\n",
      "         [ 0.9196, -0.1436,  0.0633,  ..., -0.2213, -0.4891, -0.3392],\n",
      "         [-0.2715, -0.1521,  0.1280,  ...,  0.8118,  0.1872, -0.0361],\n",
      "         [ 0.5228,  0.3717, -0.0870,  ...,  0.1787, -0.3445, -0.2823]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "Shape: torch.Size([1, 14, 768])\n"
     ]
    }
   ],
   "source": [
    "encoder_outputs = model.encoder(model.embeddings(encoding[\"input_ids\"]))\n",
    "print(\"Output:\",encoder_outputs.last_hidden_state)\n",
    "print(\"Shape:\",encoder_outputs.last_hidden_state.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we inspect the shape we see that it's the same shape as the last one! This is the origin of the name \"transformer\", it takes one sequence as input and outputs a sequence with the same shape. These are called \"context embeddings\" which we'll talk about at the end of this notebook. There's more than this `last_hidden_state` in the model's output and the generator system that creates text uses all of it. We, however, do not care about most of this and discard everything in the next step. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooler\n",
    "\n",
    "The last step in the forward pass of BERT for what we're doing is a pooling step. This gives us the final output which can be fed into other models.\n",
    "\n",
    "Roughly, BERT was initially trained to do text generation, predict the next token in the sequence. This is very easy to set up, all you need is a metric boatload of text. GPT3 was trained on trillions of tokens scraped from all over the web. To give you context, there are only 2.3 billion tokens in all of Wikipedia. BERT was trained on billions of tokens, an is a small model by today's standards.\n",
    "\n",
    "Once this initial training was done, then BERT was trained to do text classification. This is harder to setup as it needs a curated dataset with sentences that are paired. The most common one is [GLUE](https://gluebenchmark.com/). This secondary training step is called finetuning, and is usually used to make a large foundational model that was trained on one task do something else. For images for example, one can finetune a model trained on ImageNet to classify images from medical scan instead. ChatGPT has extra functionality that probably required a very interesting training pipeline with a few more finetuning steps.\n",
    "\n",
    "Here's the pooling step in BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8019, -0.3828, -0.8803,  0.4781,  0.6283, -0.1234,  0.7109,  0.2231,\n",
      "         -0.6607, -0.9999, -0.2453,  0.7718,  0.9811,  0.5380,  0.9193, -0.6514,\n",
      "         -0.0960, -0.5760,  0.1937,  0.0498,  0.6715,  1.0000,  0.0885,  0.2818,\n",
      "          0.4310,  0.9341, -0.7703,  0.9159,  0.9443,  0.6812, -0.5385,  0.1731,\n",
      "         -0.9905, -0.2347, -0.9173, -0.9890,  0.3910, -0.6985,  0.0112,  0.2179,\n",
      "         -0.8985,  0.2892,  0.9999, -0.5019,  0.2948, -0.3317, -1.0000,  0.2828,\n",
      "         -0.8458,  0.7528,  0.8369,  0.5738, -0.0594,  0.3531,  0.4162,  0.0565,\n",
      "         -0.2063, -0.0427, -0.2212, -0.5139, -0.6546,  0.4094, -0.7918, -0.8546,\n",
      "          0.7591,  0.7553, -0.0725, -0.2874,  0.0137, -0.1024,  0.7832,  0.2025,\n",
      "         -0.1078, -0.8899,  0.5245,  0.2641, -0.6041,  1.0000, -0.1185, -0.9764,\n",
      "          0.8874,  0.6552,  0.5250, -0.1935,  0.5401, -1.0000,  0.4399, -0.1549,\n",
      "         -0.9904,  0.1935,  0.4651, -0.1812,  0.7752,  0.5038, -0.7092, -0.3990,\n",
      "         -0.1677, -0.8439, -0.2306, -0.1749, -0.0436, -0.1631, -0.3401, -0.3686,\n",
      "          0.3111, -0.4396, -0.2331,  0.3944, -0.0050,  0.6450,  0.3638, -0.3282,\n",
      "          0.4400, -0.9457,  0.5275, -0.3010, -0.9862, -0.5036, -0.9896,  0.6396,\n",
      "         -0.2517, -0.2818,  0.9409, -0.1836,  0.4309, -0.0317, -0.7310, -1.0000,\n",
      "         -0.3961, -0.4735, -0.2222, -0.2269, -0.9728, -0.9620,  0.5241,  0.9408,\n",
      "          0.1559,  0.9998, -0.1309,  0.9446, -0.1606, -0.4878,  0.3625, -0.4051,\n",
      "          0.7179, -0.1348, -0.3412,  0.1762, -0.1151,  0.0848, -0.6240, -0.1262,\n",
      "         -0.6864, -0.9308, -0.3259,  0.9437, -0.3963, -0.8508, -0.0221, -0.0988,\n",
      "         -0.3105,  0.7629,  0.5881,  0.2835, -0.4163,  0.3115,  0.2753,  0.3912,\n",
      "         -0.7060,  0.2994,  0.3839, -0.2673, -0.8632, -0.9811, -0.2649,  0.5108,\n",
      "          0.9876,  0.6798,  0.2868,  0.5684, -0.1300,  0.4729, -0.9580,  0.9818,\n",
      "         -0.2168,  0.2749, -0.5357,  0.5109, -0.7908,  0.3412,  0.6303, -0.4163,\n",
      "         -0.7662, -0.0807, -0.4335, -0.2206, -0.8303,  0.2861, -0.2290, -0.2995,\n",
      "         -0.0213,  0.9410,  0.8802,  0.5341,  0.2129,  0.4808, -0.8436, -0.3512,\n",
      "         -0.0330,  0.1191,  0.1631,  0.9909, -0.7130,  0.0400, -0.9156, -0.9822,\n",
      "         -0.1703, -0.7927, -0.1656, -0.5881,  0.5869, -0.3130,  0.2722,  0.3918,\n",
      "         -0.8609, -0.6646,  0.2915, -0.4029,  0.4633, -0.1774,  0.9158,  0.9072,\n",
      "         -0.6091,  0.2599,  0.9376, -0.8929, -0.7014,  0.5011, -0.1685,  0.7297,\n",
      "         -0.5311,  0.9782,  0.7836,  0.6251, -0.9129, -0.7882, -0.6508, -0.6005,\n",
      "         -0.0691,  0.1465,  0.8411,  0.5394,  0.3791,  0.1920, -0.4577,  0.9640,\n",
      "         -0.9764, -0.9511, -0.8499,  0.0463, -0.9883,  0.8408,  0.3850,  0.3477,\n",
      "         -0.4799, -0.5082, -0.9595,  0.6753,  0.0521,  0.9356, -0.4265, -0.7277,\n",
      "         -0.5001, -0.9377, -0.1558, -0.1147, -0.3070, -0.1374, -0.9390,  0.4685,\n",
      "          0.4156,  0.4305, -0.7872,  0.9897,  1.0000,  0.9735,  0.8430,  0.7135,\n",
      "         -0.9997, -0.8030,  1.0000, -0.9725, -1.0000, -0.8956, -0.5939,  0.2180,\n",
      "         -1.0000, -0.1977,  0.1247, -0.9161,  0.4922,  0.9635,  0.9287, -1.0000,\n",
      "          0.7923,  0.8989, -0.4996,  0.8501, -0.3426,  0.9632,  0.6217,  0.4692,\n",
      "         -0.1238,  0.4371, -0.9511, -0.7213, -0.2981, -0.7789,  0.9957,  0.0717,\n",
      "         -0.5045, -0.8739,  0.3269, -0.1853, -0.2135, -0.9669, -0.2463,  0.3136,\n",
      "          0.7447,  0.0474,  0.3730, -0.5163,  0.2404,  0.0157,  0.1567,  0.5965,\n",
      "         -0.9177, -0.2658,  0.1665, -0.4054, -0.6480, -0.9719,  0.9524, -0.3728,\n",
      "          0.5705,  1.0000,  0.2849, -0.7822,  0.6301,  0.1782, -0.4026,  1.0000,\n",
      "          0.8179, -0.9815, -0.4355,  0.6601, -0.5594, -0.5166,  0.9990, -0.1530,\n",
      "         -0.5917, -0.0921,  0.9841, -0.9861,  0.9947, -0.7896, -0.9651,  0.9622,\n",
      "          0.9309, -0.7050, -0.5856,  0.0699, -0.6490,  0.2135, -0.8599,  0.4785,\n",
      "          0.3608, -0.0073,  0.8554, -0.3468, -0.4962,  0.1936, -0.4761,  0.1287,\n",
      "          0.9116,  0.3900, -0.1319, -0.0415, -0.3931, -0.7432, -0.9599,  0.4217,\n",
      "          1.0000, -0.1700,  0.6925, -0.4488, -0.0083, -0.0293,  0.5528,  0.5324,\n",
      "         -0.2768, -0.8219,  0.7004, -0.8970, -0.9917,  0.5478,  0.2137, -0.1952,\n",
      "          0.9999,  0.5326,  0.1247,  0.4774,  0.9187, -0.1269,  0.4190,  0.8134,\n",
      "          0.9761, -0.2453,  0.4384,  0.6181, -0.8164, -0.3340, -0.5914,  0.0573,\n",
      "         -0.9254,  0.2284, -0.9506,  0.9599,  0.8461,  0.3203,  0.2122,  0.5288,\n",
      "          1.0000, -0.9019,  0.4721,  0.5856,  0.3216, -0.9996, -0.6873, -0.3594,\n",
      "         -0.0255, -0.7602, -0.3685,  0.2365, -0.9610,  0.6366,  0.3696, -0.9416,\n",
      "         -0.9855, -0.1027,  0.4709,  0.0580, -0.9728, -0.6281, -0.5577,  0.2976,\n",
      "         -0.1353, -0.9377,  0.0094, -0.3011,  0.2924, -0.2183,  0.4998,  0.7994,\n",
      "          0.6641, -0.5920, -0.2930, -0.0917, -0.7472,  0.7199, -0.5473, -0.8289,\n",
      "         -0.1810,  1.0000, -0.3174,  0.8682,  0.6107,  0.4230, -0.0761,  0.2329,\n",
      "          0.9168,  0.3375, -0.7793, -0.6216,  0.3866, -0.2538,  0.5938,  0.5689,\n",
      "          0.6957,  0.7652,  0.7181,  0.1046, -0.0058, -0.1092,  0.9914,  0.0772,\n",
      "         -0.0687, -0.3145,  0.0836, -0.3153,  0.5114,  1.0000,  0.2014,  0.3439,\n",
      "         -0.9891, -0.7899, -0.7852,  1.0000,  0.8612, -0.6376,  0.5495,  0.5925,\n",
      "         -0.1183,  0.5427, -0.1830, -0.3416,  0.2307,  0.1248,  0.9439, -0.6051,\n",
      "         -0.9773, -0.5538,  0.3315, -0.9561,  0.9999, -0.4699, -0.1917, -0.4639,\n",
      "         -0.0375, -0.7966, -0.1208, -0.9835,  0.0346,  0.1963,  0.9498,  0.2459,\n",
      "         -0.4563, -0.7836,  0.6440,  0.6210, -0.8415, -0.9387,  0.9583, -0.9718,\n",
      "          0.5564,  1.0000,  0.3927, -0.3621,  0.1246, -0.3724,  0.3182, -0.5439,\n",
      "          0.4665, -0.9454, -0.3735, -0.2155,  0.1852,  0.0184, -0.4576,  0.5676,\n",
      "          0.1652, -0.4624, -0.6416,  0.0310,  0.3563,  0.6843, -0.1100, -0.0650,\n",
      "         -0.0704,  0.0157, -0.8918, -0.3196, -0.3997, -1.0000,  0.6087, -1.0000,\n",
      "          0.4038,  0.1899, -0.1382,  0.7744,  0.6736,  0.4077, -0.6746, -0.7166,\n",
      "          0.5531,  0.7016, -0.1797, -0.0774, -0.5189,  0.3072, -0.0946,  0.2149,\n",
      "         -0.2713,  0.7176, -0.1643,  1.0000,  0.1316, -0.5263, -0.8630,  0.2003,\n",
      "         -0.1884,  1.0000, -0.6880, -0.9432,  0.4355, -0.6041, -0.7857,  0.3093,\n",
      "          0.0185, -0.7430, -0.8793,  0.8916,  0.6615, -0.6013,  0.4592, -0.2728,\n",
      "         -0.3688, -0.0890,  0.7600,  0.9869,  0.5584,  0.6602, -0.3030, -0.1963,\n",
      "          0.9629,  0.1345, -0.1764, -0.0158,  1.0000,  0.3200, -0.8470,  0.1469,\n",
      "         -0.9454, -0.2731, -0.8968,  0.3120,  0.2128,  0.8980, -0.1519,  0.9312,\n",
      "         -0.5765, -0.0776, -0.5048, -0.2524,  0.2188, -0.9261, -0.9856, -0.9835,\n",
      "          0.4981, -0.3484,  0.0428,  0.2340, -0.0851,  0.3111,  0.4625, -1.0000,\n",
      "          0.9142,  0.3505,  0.8983,  0.9549,  0.7333,  0.3631,  0.2808, -0.9797,\n",
      "         -0.9230, -0.3366, -0.1397,  0.5823,  0.5019,  0.8711,  0.2694, -0.5318,\n",
      "         -0.4342, -0.4096, -0.9479, -0.9936,  0.4175, -0.2006, -0.7937,  0.9620,\n",
      "         -0.1627, -0.0927,  0.0358, -0.7700,  0.6894,  0.6933,  0.0780,  0.0951,\n",
      "          0.2567,  0.8473,  0.8565,  0.9708, -0.7465,  0.6075, -0.3542,  0.5413,\n",
      "          0.9085, -0.9328,  0.0978,  0.3286, -0.4174,  0.1888, -0.1954, -0.8364,\n",
      "          0.6715, -0.2638,  0.5988, -0.2657,  0.0758, -0.3671, -0.0481, -0.5898,\n",
      "         -0.5487,  0.5860,  0.2613,  0.8665,  0.8685, -0.0564, -0.3390, -0.1441,\n",
      "         -0.7380, -0.9368,  0.6679,  0.0383, -0.3862,  0.5905, -0.0663,  0.9259,\n",
      "          0.2274, -0.3071, -0.2814, -0.6295,  0.7703, -0.3439, -0.5731, -0.4625,\n",
      "          0.6168,  0.3218,  1.0000, -0.7633, -0.8295, -0.3010, -0.3610,  0.4506,\n",
      "         -0.4721, -1.0000,  0.3256, -0.3650,  0.6894, -0.5710,  0.8500, -0.2129,\n",
      "         -0.9433, -0.2798,  0.5800,  0.6407, -0.4569, -0.2627,  0.4716,  0.4002,\n",
      "          0.9373,  0.7286, -0.0680,  0.0815,  0.5651, -0.6884, -0.6592,  0.8994]],\n",
      "       grad_fn=<TanhBackward0>)\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "bert_outputs = model.pooler(encoder_outputs.last_hidden_state)\n",
    "print(bert_outputs)\n",
    "print(bert_outputs.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pooler operates on the first token and passes it through a fully connected feed forward network. This corresponds to the `[CLS]` token, and this is what is then fed to the image generation part of StableDiffusion. The other tokens aren't useful to us and are discarded in this step. \n",
    "\n",
    "## The Other Tokens\n",
    "\n",
    "The output is called the \"context embeddings\", they in a sense have added the context to the initial embeddings. . This [paper](https://proceedings.neurips.cc/paper/2019/file/159c1ffe5b61b41b3c4d8f4c2150f6c4-Paper.pdf) has an excellent example with the token for \"die\" which we will talk about. \n",
    "\n",
    "![German Die vs English die](bert_die_vs_die.png)\n",
    "\n",
    "The final tokens in a sense have the \"in-context\" meaning when the model is trained to replace masked words. The image is from the paper where they show that the token \"die\" is embedded in 4 separate locations depending on if it's the German article \"die\" that means \"the\" in english, or the english verb \"die\" in it's singular or plural form, or the game piece you roll while playing D&D or craps.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_dissection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae26f472e0f4ed7772121e90f50617fd41a7ce432ded5fa547fe40d1565754e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
